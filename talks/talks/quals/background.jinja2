<section class="center">
  <h2 class="center">3. Background and Prior Work</h2>
  <ul>
    <li class="fragment">An overview of simulation broadly
      <ul><li>Abstractions used in hardware modeling</li></ul>
    </li>
    <li class="fragment">The limitations of existing microarchitectural simulators</li>
    <li class="fragment">Sampled simulation techniques
      <ul>
        <li>Phase behavior of programs</li>
        <li>SimPoint: Representative sampling</li> <!-- interval embedding + clustering -->
        <li>SMARTs: Reservoir sampling</li> <!-- error bounding via CLT -->
        <li>Functional warmup techniques</li>
        <li>Detailed warmup</li>
      </ul>
    </li>
    <li class="fragment">Prior work on rapid microarchitectural evaluation</li>
  </ul>
</section>

<section>
  <h2>A Broad View of Simulation</h2>

  <div class="center">
    <img src="./figs/dynamic/tidalsim/simulators_broadly.svg" />
    <figcaption class="small center">A high-level, generic view of the input and outputs of a simulator.</figcaption>
  </div>

  <ul style="margin-top: 1rem;">
    <li class="fragment">Simulation is the workhorse of architecture evaluation</li>
    <li class="fragment">Simulation inputs can have wide variation of fidelity
      <ul>
        <li><strong>Hardware spec</strong>: high-level models to detailed microarchitecture</li>
        <li><strong>Workload</strong>: high-level algorithmic description to concrete binary</li>
      </ul>
    </li>
    <li class="fragment">The fidelity of simulation outputs tracks that of the inputs</li>
    <!--<li>Inputs: architecture specification or framework, workload
      <ul>
        <li>Specification can be high-level (blocks + instruction spec + latencies) or very detailed</li>
        <li>Workload can be high-level (memory access pattern) or fully concrete</li>
      </ul>
    </li>
    <li>Outputs: execution trace/metrics - how does the architecture execute the workload? - notion of time granularity and space granularity + PPA stuff if available </li>-->
  </ul>
</section>

<section>
  <h2>Hardware Abstractions</h2>

  <p class="center fragment">There are roughly <strong>4 levels of hardware abstractions</strong> used in architecture evaluation</p>

  <ol>
    <li class="fragment">Architectural (functional) models</li>
    <li class="fragment">Microarchitectural models (<em>"rough"</em>) with approximate state and timing</li>
    <li class="fragment">Microarchitectural models (<em>"detailed"</em>) with more refined state and timing</li>
    <li class="fragment">Register-transfer level (RTL) models with full fidelity state and timing</li>
  </ol>
</section>

<section>
  <h2>Hardware Abstractions: Architectural (Functional) Models</h2>

  <p class="fragment center">Functional simulators emulate an architecture and can execute workloads (e.g. ELF binaries).</p>
  <p class="fragment center">They only model <em>architectural</em> state, defined in an ISA specification. No <em>microarchitectural</em> state is modeled.</p>

  <ul>
    <li class="fragment">Examples
      <ul>
        <li>RISC-V: <a href="https://github.com/riscv-software-src/riscv-isa-sim">spike</a>, <a href="https://github.com/chipsalliance/dromajo">dromajo</a>, <a href="https://www.imperas.com/riscvovpsim-free-imperas-risc-v-instruction-set-simulator">Imperas riscvOVPSim</a></li>
        <li>Multi-ISA (x86, ARM, RISC-V): <a href="https://www.qemu.org/">qemu</a></li>
      </ul>
    </li>
    <li class="fragment">Very fast (10-1000 MIPS), low startup latency (instant)</li>
    <li class="fragment">Cannot estimate PPA</li>
  </ul>
</section>

<section>
  <h2>Hardware Abstractions: "Rough" uArch Models - Micro-Kernel Accelerators</h2>

  <ul class="smallish">
    <li class="fragment">Microarchitecture is modeled with high-level blocks with a dataflow and timing relationship</li>
    <li class="fragment">Prior Work: Aladdin<sup>[1]</sup> is a microarchitecture estimation and simulation tool for analyzing the PPA of potential accelertors for kernels written in C</li>
  </ul>

  <div class="center r-stack">
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="50%" src="./figs/quals/aladdin_architecture.png">
      <figcaption class="small center">System architecture assumed by Aladdin</figcaption>
    </div>
    <div class="fragment fade-in" style="display: grid; place-items: center;">
      <img width="70%" src="./figs/quals/aladdin_flow.png">
      <figcaption class="small center">Aladdin flow. Inputs: workload. Outputs: dataflow microarchitecture + PPA</figcaption>
    </div>
    <!--<div class="fragment fade-in" style="display: grid; place-items: center;">
      <img width="20%" src="./figs/quals/aladdin_simulation.png">
      <figcaption class="small center">Aladdin simulation</figcaption>
    </div>-->
  </div>

  <div class="fragment">
  <hr>
  <div class="verysmall">
    <p class="footnote">
    [1]: Shao, Y.S., Reagen, B., Wei, G.Y. and Brooks, D., 2014. Aladdin: A pre-rtl, power-performance accelerator simulator enabling large design space exploration of customized architectures. ACM SIGARCH.
    </p>
  </div>
  </div>
</section>

<section>
  <h2>Hardware Abstractions: "Rough" uArch Models - ML Accelerators</h2>

  <ul class="small">
    <li class="fragment">Rough uArch models are used for evaluating ML accelerator architectures, dataflows, and workload mappings</li>
    <li class="fragment">Prior Work: Timeloop<sup>[1]</sup>, Accelergy<sup>[2]</sup> provides a framework for describing accelerator microarchitecture with parameterizable blocks (PEs, scratchpads), workload mappings, and simulating workloads for PPA estimates</li>
  </ul>

  <div class="center r-stack">
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="50%" src="./figs/quals/accelergy_example_arch.png" style="margin:0;">
      <figcaption class="small center">An example microarchitecture modeled by Timeloop</figcaption>
    </div>
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="50%" src="./figs/quals/timeloop_arch_definition.png" style="margin:0;">
      <figcaption class="small center">Microarchitecture description schema provided by Timeloop</figcaption>
    </div>
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="30%" src="./figs/quals/timeloop_workload_definition.png" style="margin:0;">
      <figcaption class="small center">Timeloop's schema for defining workloads and their mapping</figcaption>
    </div>
    <div class="fragment fade-in" style="display: grid; place-items: center;">
      <img width="70%" src="./figs/quals/timeloop_outputs.png" style="margin:0;">
      <figcaption class="small center">Timeloop + Accelergy flow. Inputs: workload. Outputs: ML microarchitecture + PPA</figcaption>
    </div>
  </div>

  <div class="fragment">
  <div class="verysmall">
    <p class="footnote" style="margin:0;">
    [1]: Parashar, A., et. al., 2019. Timeloop: A systematic approach to dnn accelerator evaluation. ISPASS.<br/>
    [2]: Wu, Y.N., Emer, J.S. and Sze, V., 2019. Accelergy: An architecture-level energy estimation methodology for accelerator designs. ICCAD.
    </p>
  </div>
  </div>
</section>

<section>
  <h2>Hardware Abstractions: "Rough" uArch Models - Cores</h2>
  <!-- Wattch, CACTI, McPAT (CPU modeling) -->

  <ul class="smallish">
    <li class="fragment">Rough uArch models are also common for evaluating core microarchitectures</li>
    <li class="fragment">Prior Work: McPAT<sup>[1]</sup> models CPUs with a parameterizable out-of-order pipeline and uncore components coupled to a timing simulator. CACTI<sup>[2]</sup> models the PPA of SRAM-based caches and DRAM.</li>
  </ul>

  <div class="center r-stack">
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="50%" src="./figs/quals/mcpat_flow.png">
      <figcaption class="small center">The simulation flow provided by McPAT.</figcaption>
    </div>
    <div class="fragment fade-in" style="display: grid; place-items: center;">
      <img width="70%" src="./figs/quals/mcpat_output.png">
      <figcaption class="small center">McPAT results. Inputs: workload and microarch description. Outputs: PPA</figcaption>
    </div>
  </div>

  <div class="fragment">
  <hr>
  <div class="verysmall">
    <p class="footnote" style="margin:0;">
    [1]: Li, S., et. al., 2009. McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures. MICRO.<br />
    [2]: Muralimanohar, et al., 2009. CACTI 6.0: A tool to model large caches. HP laboratories.
    </p>
  </div>
  </div>
</section>

<section>
  <h2>Hardware Abstractions: "Detailed" uArch Models - Cores</h2>

  <!-- Modeling precise microarchitectural details usually at cycle-level time-granularity -->
  <!-- Workload type - concrete binary -->

  <ul class="smallish">
    <li class="fragment">The most popular way to evaluate core microarchitectural optimizations is with a detailed execution-driven simulator. Many microarchitectural states are modeled.</li>
    <li class="fragment">Prior Work: gem5, ZSim, SST, MARSSx86, Sniper, ESESC. These simulators model the core pipeline and uncore components with cycle-level time-granularity.</li>
  </ul>

  <div class="center r-stack">
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="50%" src="./figs/quals/gem5_architecture.png">
      <figcaption class="small center">The modular architecture of gem5.</figcaption>
    </div>
    <div class="fragment fade-in" style="display: grid; place-items: center;">
      <img width="80%" src="./figs/quals/gem5_kanata_pipeline_viewer.png">
      <figcaption class="small center">Detailed per-instruction core pipeline visualization using Konata.</figcaption>
    </div>
  </div>
</section>

<section>
  <h2>Hardware Abstractions: RTL</h2>

  <ul>
    <li class="fragment">Register-transfer level (RTL) (e.g. Verilog) is the lowest abstraction used in pre-silicon architecture evaluation</li>
    <li class="fragment">Every bit of state and logic is explicitly modeled. RTL is the highest fidelity hardware model.</li>
    <li class="fragment">Can extract very precise power, performance, and area metrics</li>
  </ul>
</section>

<section>
  <h2>Which Hardware Abstraction is Suitable?</h2>

  <ol class="small">
    <li>Architectural (functional) models</li>
    <li>Microarchitectural models (<em>"rough"</em>) with approximate state and timing</li>
    <li>Microarchitectural models (<em>"detailed"</em>) with more refined state and timing</li>
    <li>Register-transfer level (RTL) models with full fidelity state and timing</li>
  </ol>

  <p class="center fragment">Can't compromise on accuracy or latency to enable meaningful and fast microarchitectural iteration.</p>

  <hr class="fragment">

  <p class="center fragment"><strong>"detailed" uArch models</strong> or <strong>RTL</strong> are the only options for our performance simulator selection</p>

</section>

<section>
  <h2>Simulator Metrics</h2>

  <p class="center fragment">Simulation techniques span the gamut on various axes. Each simulation technique assumes <em>a particular hardware abstraction</em>.</p>

  <ul class="smallish">
    <li class="fragment"><strong>Throughput</strong>
      <ul><li>How many instructions can be simulated per real second? (MIPS = millions of instructions per second)</li></ul>
    </li>
    <li class="fragment"><strong>Accuracy</strong>
      <ul><li>Do the output metrics of the simulator match those of the modeled SoC in its real environment?</li></ul>
    </li>
    <li class="fragment"><strong>Startup latency</strong>
      <ul><li>How long does it take from the moment the simulator's parameters/inputs are modified to when the first instruction is executed?</li></ul>
    </li>
    <li class="fragment"><strong>Metric diversity</strong>
      <ul>
        <li>What metrics can the simulator emit?</li>
        <li>This is tied to the hardware abstraction used</li>
      </ul>
    </li>
    <li class="fragment"><strong>Cost</strong>
      <ul>
        <li>What hardware platform does the simulator run on?</li>
        <li>How much does it cost to run a simulation?</li>
      </ul>
    </li>
  </ul>
</section>

<section>
  <!--RTL vs detailed uArch vs arch vs high-level spec

  We will restrict our scope to architectures which already have concrete implementations.
  No need to high-level analytical analysis since we're trying to speed up iteration loop, not discover new accelerator or core architectures.
  This is a separate research direction.-->
  <h2>Existing Hardware Evaluation Techniques</h2>

  <table class="comparison_table">
    <thead><tr>
      <th></th>
      <th>Examples</th>
      <th>Throughput</th>
      <th>Latency</th>
      <th>Accuracy</th>
      <th>Metrics</th>
      <th>Cost</th>
    </tr></thead>
    <tbody>
    <tr class="fragment">
      <td>Analytical Models</td>
      <td>Aladdin, Timeloop</td>
      <td class="bg-green">10-1000 MIPS</td>
      <td class="bg-green">seconds</td>
      <td class="bg-red">Wildly variable</td>
      <td>PPA estimates</td>
      <td class="bg-green">Minimal</td>
    </tr>
    <tr class="fragment">
      <td>Architectural Simulators</td>
      <td>spike, qemu</td>
      <td class="bg-green">10-100+ MIPS</td>
      <td class="bg-green">&lt;1 second</td>
      <td class="bg-red">None</td>
      <td>Commit trace</td>
      <td class="bg-green">Minimal</td>
    </tr>
    <tr class="fragment">
      <td>μArch Simulators</td>
      <td>gem5, Sniper, ZSim, SST</td>
      <td class="bg-orange">100 KIPS (gem5) - 100 MIPS (Sniper)</td>
      <td class="bg-green">&lt;1 minute</td>
      <td class="bg-orange">10-50% IPC error</td>
      <td>Pipeline view / IPC trace</td>
      <td class="bg-green">Minimal</td>
    </tr>
    <tr class="fragment">
      <td>RTL Simulators</td>
      <td>Verilator, VCS, Xcelium</td>
      <td class="bg-red">1-10 KIPS</td>
      <td class="bg-orange">2-10 minutes</td>
      <td class="bg-green">Cycle-exact</td>
      <td>RTL-level traces</td>
      <td class="bg-green">Minimal</td>
    </tr>
    <tr class="fragment">
      <td>FPGA Prototypes</td>
      <td>HAPS, Protium</td>
      <td class="bg-red">≈ 50 MIPS</td>
      <td class="bg-red">2-6 hours</td>
      <td class="bg-green">Cycle-exact</td>
      <td>Subset of RTL signals</td>
      <td class="bg-orange">$10k+</td>
    </tr>
    <tr class="fragment">
      <td>FPGA-Based Emulators</td>
      <td>Firesim</td>
      <td class="bg-green">≈ 10 MIPS</td>
      <td class="bg-red">2-6 hours</td>
      <td class="bg-green">Cycle-exact</td>
      <td>Subset of RTL signals</td>
      <td class="bg-orange">$10k+</td>
    </tr>
    <tr class="fragment">
      <td>ASIC-Based Emulators</td>
      <td>ZeBu, Palladium</td>
      <td class="bg-green">≈ 1-10 MIPS</td>
      <td class="bg-orange">&lt;1 hour</td>
      <td class="bg-green">Cycle-exact</td>
      <td>RTL-level traces</td>
      <td class="bg-red">$10M+</td>
    </tr>
    <tr class="fragment">
      <td>Multi-level Sampled Simulation</td>
      <td><strong>TidalSim</strong></td>
      <td class="bg-green">10+ MIPS</td>
      <td class="bg-green">&lt;1 minute</td>
      <td class="bg-green">&lt;1% IPC error</td>
      <td>Sampled RTL-level traces</td>
      <td class="bg-green">Minimal</td>
    </tr>
    </tbody>
  </table>
  <!-- uArch Perf Sim is nearly there! Can we improve its accuracy or improve its throughput to get us to the golden promise land? -->
</section>

<section>
  <h2>Can we Use Microarchitectural Simulators?</h2>
  <!-- Consider accuracy vs speed tradeoff + implementation complexity tradeoff -->

  <!-- - cite paper on arch sims considered harmful
  Accuracy - discuss why this can't be really improved - need to resort to RTL for fidelity that we can trust (on PPA)
  Throughput - we can use sampling - let's discuss that -->

  <!-- <p class="fragment center">If we can make uArch simulators <strong>faster</strong> (10x off from functional simulators) and <strong>more accurate</strong>, then the problem is solved!</p>-->

  <ul>
    <li class="fragment">uArch simulators seem to satisfy most of our requirements
      <ul>
        <li class="fragment"><strong>Low startup latency</strong>: seconds to 1 minute</li>
        <li class="fragment"><Strong>Metrics</strong>: IPC traces
        <li class="fragment"><strong>Cost</strong>: minimal</li>
      </ul>
    </li>
    <li class="fragment">Can we adapt uArch simulators to perform better in terms of <strong>accuracy</strong> and <strong>throughput</strong>?</li>
  </ul>
</section>

<section>
  <h2>Accuracy of Microarchitectural Simulators</h2>

  <!-- Want to show:
    - simulators are generally inaccurate wrt real hardware first of all
    - simulators disagree with each other a lot
    - simulators poorly model the impact of various uarch changes (pipeline width, cache size, branch predictor)
    - simulators need calibration with silicon to get decent accuracy (which is painful and doesn't prove anything about generalizability)
  -->

  <div class="center r-stack">
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="80%" src="./figs/quals/uarch_simulator_study-ipc.png" />
      <figcaption class="small center">
      Comparison of estimated IPC from various uArch simulators vs real IPC from Haswell.<sup>[1]</sup>
      MAPE on MiBench: 9.5% (Sniper), 44.6% (gem5), 38.2% (PTLSim) and 47.06% (Multi2Sim).
      </figcaption>
    </div>
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="80%" src="./figs/quals/uarch_simulator_study-ipc2.png" />
      <figcaption class="small center">Raw IPC errors on 64-bit workloads vs real Haswell<sup>[2]</sup>. Simulators not only disagree with each other, but have substantial errors exceeding 20%.
      </figcaption>
    </div>
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="80%" src="./figs/quals/uarch_simulator_study-pipeline_width.png" />
      <figcaption class="small center">Impact of halving the pipeline width (widths of fetch, decode, issue/rename, dispatch, and commit are halved)<sup>[2]</sup>. Simulators disagree with each other.
      </figcaption>
    </div>
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="80%" src="./figs/quals/uarch_simulator_study-reduced_cache_size.png" />
      <figcaption class="small center">Impact of halving all the cache sizes<sup>[2]</sup>. Note how MARSSx86 shows <em>increased</em> IPC for some benchmarks! Again, disagreements are substantial.
      </figcaption>
    </div>
    <div class="fragment fade-in" style="display: grid; place-items: center;">
      <img width="80%" src="./figs/quals/uarch_simulator_study-branch_predictor.png" />
      <figcaption class="small center">Impact of using a bimodal branch predictor vs the Haswell BP.<sup>[2]</sup>.
      The sensitivity of each simulator is wildly different!
      </figcaption>
    </div>
  </div>

  <!-- Trends aren't enough - see the sensitivity differences of these simulators! Gradients are critical! -->

  <p class="smallish fragment center">uArch simulators are <strong>not accurate enough</strong> for microarchitectural iteration.</p>
  <p class="smallish fragment center">Trends aren't enough. Note the sensitivity differences - gradients are critical!</p>

  <div class="fragment">
  <hr>
  <div class="verysmall">
    <p class="footnote" style="margin:0;">
    [1]: Akram, A. and Sawalha, L., 2016, October. x86 computer architecture simulators: A comparative study. ICCD.<br />
    [2]: Akram, A. and Sawalha, L., 2019. A survey of computer architecture simulation techniques and tools. IEEE Access
    </p>
  </div>
  </div>
</section>

<section>
  <h2>Flexibility vs Accuracy Tradeoff of uArch Simulators</h2>

  <blockquote class="small">
  Sniper though shows greater accuracy, is not very flexible to allow one to model new micro-architectural features compared to gem5.
  <!--Sniper does not support full-system simulation and the effect of OS for applications.
  It is best use is for x86 many-core user-mode workloads.-->
  On the other hand, gem5 and PTLsim are more flexible and can be used for studies of particular microarchitectural blocks, and full-system workloads, with gem5 being more configurable and showing higher accuracy. [1]
  </blockquote>

  <p class="center fragment">There is an unfavorable tradeoff of simulator flexibility (due to precise silicon calibration) and accuracy.</p>

  <div class="fragment">
  <hr>
  <div class="verysmall">
    <p class="footnote" style="margin:0;">
    [1]: Akram, A. and Sawalha, L., 2019. A survey of computer architecture simulation techniques and tools. IEEE Access<br/>
    </p>
  </div>
  </div>
</section>

<section>
  <h2>The Trends Myth</h2>
  <blockquote class="small">
It is casually stated as, “Although specific details of the simulation are wrong, the overall trends will be correct.” <br />

Relative performance comparisons may be correct, even if there is absolute error caused by a poor assumption or bug.<sup>[1]</sup>
  </blockquote>

  <blockquote class="small">
However, for this to be true, the new technique being evaluated through simulation must be insulated from or statistically uncorrelated with the source of simulation errors.

Because simulators can have significant errors, which are completely unknown, only in rare cases can we be sure this argument holds.<sup>[1]</sup>
  </blockquote>

  <p class="center fragment">
    Even accurate relative trends are not enough for microarchitectural iteration - the gradients must also be precise!
  </p>

  <div class="fragment">
  <hr>
  <div class="verysmall">
    <p class="footnote" style="margin:0;">
    [1]: Nowatzki, T., Menon, J., Ho, C.H. and Sankaralingam, K., 2015. Architectural simulators considered harmful. IEEE Micro.
    </p>
  </div>
  </div>
</section>

<section>
  <h2>Accuracy of gem5 for RISC-V Cores</h2>
  <div class="center r-stack">
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="40%" src="./figs/quals/gem5_riscv_rsd_ipc.png" />
      <figcaption class="small center">
      Cycles ratio of custom + 3 MiBench (qsort, stringsearch) baremetal binaries on RSD (OoO RV32IMF RISC-V core) vs a gem5 model matching the core microarchitecture parameters<sup>[1]</sup>.
      Up to 40% IPC error.
      </figcaption>
    </div>
    <div class="fragment fade-in-then-out" style="display: grid; place-items: center;">
      <img width="40%" src="./figs/quals/gem5_riscv_rsd_mpki.png" />
      <figcaption class="small center">
      Significant mismatches in L1 MPKI/MAKI are main contributor to error.<sup>[1]</sup>
      </figcaption>
    </div>
    <div class="fragment fade-in" style="display: grid; place-items: center;">
      <img width="50%" src="./figs/quals/gem5_calibration_slide.png" />
      <figcaption class="small center">
      The complexity and effort to 'calibrate' a uArch simulator to RTL simulation is significant.<sup>[2]</sup>
      </figcaption>
    </div>
  </div>

  <p class="fragment center">There is <strong>no evidence</strong> that uArch simulators can achieve sub 5% accuracy for microarchitectural iteration</p>

  <div class="fragment">
  <hr>
  <div class="verysmall">
    <p class="footnote" style="margin:0;">
    [1]: Chatzopoulos, O., et. al., 2021. Towards Accurate Performance Modeling of RISC-V Designs. arXiv preprint<br />
    [2]: Ta, T., Cheng, L. and Batten, C., 2018. Simulating multi-core RISC-V systems in gem5. CARRV
    </p>
  </div>
  </div>
</section>

<section>
  <h2>But Can't We Calibrate uArch Simulators?</h2>

  <!--<p class="fragment center">There is a body of work on 'calibrating' uArch simulators against silicon, but very scant work on calibrating against RTL generators</p>-->

  <ul class="smallish">
    <li class="fragment">[1] calibrates Sniper to Cortex A54 and A72 cores with average IPC errors of 7% and 15% <small>(up to 50%)</small> on SPEC CPU2017 respectively using ML to fine-tune model parameters against silicon measurements given microbenchmarks.</li>
    <li class="fragment">[2] calibrates MARSSx86 to i7-920 with post-calibration IPC error of 7% on SPEC</li>
  </ul>

  <hr class="fragment">

  <ul class="smallish">
    <li class="fragment">Absolute errors are still too high when architects must make decisions based on tiny IPC changes</li>
    <li class="fragment">Calibration only applies to a specific design point!</li>
    <li class="fragment">Gradients and errors are not understood nor bounded when microarchitecture parameters are altered to perform HW parameter DSE</li>
  </ul>

  <p class="center fragment">uArch simulators are not suitable for pre-silicon microarchitectural iteration</p>

  <div class="fragment">
  <hr>
  <div class="verysmall">
    <p class="footnote" style="margin:0;">
    [1]: Adileh, A., et al., 2019. Racing to hardware-validated simulation. ISPASS.<br />
    [2]: Asri, M., et al., 2016. Simulator calibration for accelerator-rich architecture studies. SAMOS
    </p>
  </div>
  </div>
</section>

<section>
  <h2>False Confidence from Validation</h2>

  <blockquote class="small">
  A common misconception is that if the parameters are changed and configured for some other design point, the accuracy will be similar.<sup>[1]</sup>
  </blockquote>

  <blockquote class="small fragment">
  tool validation is often carried out by fitting parameters to the “specific” validation targets, not about ensuring theunderlying modeling is accurate for individual phenomena or their interactions.<sup>[1]</sup>
  </blockquote>

  <p class="fragment center">Calibration / validation of a uArch simulator against silicon doesn't make it suitable for microarchitectural iteration</p>

  <div class="fragment">
  <hr>
  <div class="verysmall">
    <p class="footnote" style="margin:0;">
    [1]: Nowatzki, T., Menon, J., Ho, C.H. and Sankaralingam, K., 2015. Architectural simulators considered harmful. IEEE Micro.
    </p>
  </div>
  </div>
</section>

<section>
  <h2>Our Decision Tree Thus Far</h2>

  <!-- uArch sims are not accurate (both absolutely, wrt relative trends and gradients, and wrt parameterization). they have unbounded modeling errors that reduce our confidence substantially. -->

  <!-- We want to use RTL simulation as the lowest level performance simulator. But we also want to have enough throughput! How???-->

  <!-- While I have been hating on uArch simulators a lot, they do offer one important lesson.
  Let's learn from sampled simulation approaches previously explored by uarch simulators. -->

  <ol>
    <li class="fragment">Need high fidelity hardware abstractions {{ rightarrow }} must use detailed uArch / RTL abstractions</li>
    <li class="fragment">Need high accuracy simulators but uArch simulators are not accurate
    <ul style="font-size: 1.4rem;">
      <li class="fragment">1) absolutely, 2) with regards to relative trends and gradients, and 3) with regards to parameterizations</li>
    </ul></li>
    <li class="fragment">Therefore, we must use <strong>RTL simulation</strong> as the lowest level performance simulator</li>
  </ol>

  <hr class="fragment">

  <p class="center fragment">But RTL simulation has <strong>low throughput</strong>!</p>
  <p class="center fragment">Let's take a technique from uArch simulators that improves their throughput</p>
</section>

<section>
  <h2>Sampled Simulation</h2>

  <p class="center fragment">Instead of running the entire program in uArch simulation, run the entire program in <em>functional simulation</em> and only run <em>samples</em> in uArch simulation</p>

  <div class="fragment center">
    <img width="60%" src="./figs/multi-level-sim/sampled_simulation0.png" />
  </div>

  <p class="fragment center">The full workload is represented by a selection of <em>sampling units</em>.</p>

  <ol class="smallish">
    <li class="fragment">How should sampling units be selected?</li> <!-- sampling methodology -->
    <li class="fragment">How can we accurately estimate the performance of a sampling unit?</li> <!-- functional and detailed warmup -->
    <li class="fragment">How can we estimate errors when extrapolating from sampling units?</li> <!-- error bounding via CLT or other heuristics -->
  </ol>
</section>

<section>
  <h2 style="font-size:1.8rem;">Representative Sampling using Phase Behavior of Programs</h2>

  <div class="container">
  <div>
  <ul style="font-size:95%">
    <li class="fragment" data-fragment-index="1">Program execution traces aren’t random
      <ul class="fragment" data-fragment-index="2">
        <li>They execute the same code again-and-again</li>
        <li>Workload execution traces can be split into <strong style="text-decoration:underline;">phases</strong> that exhibit similar μArch behavior</li>
      </ul>
    </li>
    <li class="fragment" data-fragment-index="4"><strong>Prior work</strong>: SimPoint
      <ul class="fragment" data-fragment-index="5">
        <li>Compute an embedding for each program interval (e.g. blocks of 100M instructions)</li>
        <li>Cluster interval embeddings using k-means</li>
        <li>Choose representative intervals from each cluster as <em>sampling units</em></li>
        <!--<li>Identify basic blocks executed in a given interval (e.g. 1M instruction intervals)</li>
        <li>Embed each interval using their ‘basic block vector’</li>
        <li>Cluster intervals using k-means</li>-->
      </ul>
    </li>
    <li class="fragment" data-fragment-index="6"><strong>Hypothesis</strong>: intervals with similar embeddings → similar μArch behaviors
      <ul class="fragment" data-fragment-index="7">
        <!--<li>Only execute unique intervals in low-level RTL simulation!</li>-->
        <li>We can extrapolate a full workload trace from running only the sampling units in uArch simulation</li>
      </ul>
    </li>
  </ul>
  </div>
  <div class="fragment" data-fragment-index="3" style="display:grid; align-content: center; justify-items:center;">
    <img src="./figs/multi-level-sim/simpoint-gzip_phases.gif" />
    <img src="./figs/multi-level-sim/simpoint-gcc_phases.gif" />
  </div>
  </div>

  <!--<ul class="small">
    <li class="fragment">Sampled simulation only executes specific intervals (sampling units) of the full workload in detailed performance simulation
    <li class="fragment">The sampling units are chosen based on embeddings or randomly</li>
  </ul>

  <div class="fragment center">
    <img width="40%" src="./figs/multi-level-sim/sampled_simulation.png" />
  </div>

  <ul class="small">
    <li class="fragment">Functional warmup initializes the long-lived uArch state (caches, TLB, branch predictor, prefetcher) in performance simulation</li>
    <li class="fragment">Detailed warmup initializes the short-lived uArch state (pipeline, ROB, LSU)</li>
    <li class="fragment">Performance metrics are collected after detailed warmup and are assumed to be representative of the sampling unit</li>
    <li class="fragment">The sampled intervals are used to reconstruct the full IPC trace of the workload</li>
  </ul>-->
</section>

<section>
  <h2>Random Sampling</h2>
  <ul>
    <li class="fragment"><strong>Prior work</strong>: "SMARTS: Accelerating Microarchitecture Simulation via Rigorous Statistical Sampling"</li>
    <li class="fragment">Before the workload is launched, the number of sampling units is determined
      <ul>
        <li>If the sample variance is too high to achieve the target confidence bound, then more sampling units must be collected</li>
      </ul>
    </li>
    <li class="fragment">Sampling units are selected either using random, reservoir, or systematic sampling</li>
    <li class="fragment">Central limit theorem is used to derive a confidence bound around the performance metrics reported by uArch simulation of the sampling units</li>
  </ul>
</section>

<section>
  <h2>Functional Warmup</h2>

  <p class="fragment center">The state from a sampling unit checkpoint is only <em>architectural</em> state. The <em>microarchitectural</em> state of the uArch simulator starts at the reset state!</p>

  <div class="fragment center">
    <img width="50%" src="./figs/multi-level-sim/sampled_simulation.png" />
  </div>

  <ul>
    <li class="fragment">We need to seed long-lived uArch state at the beginning of each sampling unit</li>
    <li class="fragment">This process is called <em>functional warmup</em></li>
  </ul>
</section>

<section>
  <h2>Importance of Functional Warmup</h2>

  <p class="center fragment">Long-lived microarchitectural state (caches, branch predictors, prefetchers, TLBs) has a substantial impact on the performance of a sampling unit</p>

  <div class="container" style="display: grid; grid-template-columns:1fr 1.4fr;">
    <figure class="fragment center" style="display: grid; align-content: center;">
      <img width="100%" src="./figs/multi-level-sim/livesim_amat_vs_warmup.png" />
      <figcaption class="small center">AMAT Error vs # of detailed warmup instructions <sup>[1]</sup></figcaption>
    </figure>

    <figure class="fragment center" style="display: grid; align-content: center;">
      <img width="100%" src="./figs/quals/warmup_mpki_plots.png" />
      <figcaption class="small center">MPKI vs warmup vs sampling unit length for different branch predictors<sup>[2]</sup></figcaption>
    </figure>
  </div>

  <div class="fragment">
  <hr>
  <div class="verysmall">
  <p class="footnote">
  [1]: Hassani, Sina, et al. "LiveSim: Going live with microarchitecture simulation." HPCA 2016.<br/>
  [2]: Eeckhout, L., 2008. Sampled processor simulation: A survey. Advances in Computers. Elsevier.
  <!--[2]: Barr, Kenneth C., et al. "Accelerating multiprocessor simulation with a memory timestamp record." ISPASS 2005.</p>-->
  </div>
  </div>
</section>

<section>
  <h2>Overview of Sampling Methodologies</h2>

  <div class="fragment center">
    <img width="50%" src="./figs/multi-level-sim/sampled_simulation.png" />
  </div>

  <table class="fragment" style="font-size: 1.2rem;">
    <thead><tr>
      <th>Sampling Technique</th>
      <th>Interval Length</th>
      <th># of Intervals Simulated</th>
      <th>Interval Selection</th>
      <th>Functional Warmup</th>
      <th>Detailed Warmup</th>
      <th>Time Granularity</th>
    </tr></thead>
    <tbody>
    <tr class="fragment">
      <td><strong>SimPoint</strong></td>
      <td>10-100M</td>
      <td>50-100</td>
      <td>BBFV + k-means</td>
      <td>Optional</td>
      <td>≈0.1-1M</td>
      <td>Interval length</td>
    </tr>
    <tr class="fragment">
      <td><strong>SMARTS</strong></td>
      <td>1-10k</td>
      <td>10k</td>
      <td>Systematic sampling</td>
      <td>Required</td>
      <td>1k</td>
      <td>Entire workload</td>
    </tr>
    <tr class="fragment">
      <td><strong>TidalSim</strong></td>
      <td>10k</td>
      <td>10-100</td>
      <td>BBFV + k-means</td>
      <td>Required</td>
      <td>1k</td>
      <td>Interval Length</td>
    </tr>
    </tbody>
  </table>
</section>

<section>
  <h2>Final Takeaways</h2>

  <ul>
    <li class="fragment">Microarchitectural iteration requires high accuracy
      <ul><li>{{ rightarrow }} we must use <strong>RTL simulation</strong> as our performance simulator</li></ul>
    </li>
    <li class="fragment">RTL simulation has low throughput
      <ul><li>{{ rightarrow }} we must employ <strong>simulation sampling techniques</strong> to combine architectural and RTL simulation to improve throughput</li></ul>
    </li>
    <li class="fragment">We can't execute long sampling units in RTL simulation
      <ul><li>{{ rightarrow }} we must use <strong>uArch functional warmup models</strong> to minimize errors due to stale uArch state</li></ul>
    </li>
    <li class="fragment">We want <em>time-domain</em> power, performance, and RTL collateral. We want the ability to extract tiny and unique benchmarks from large workloads.
      <ul><li>{{ rightarrow }} we must <strong>combine the SimPoint and SMARTS</strong> sampling methodologies</li></ul>
    </li>
  </ul>
</section>
<!--
  - need high accuracy: we must use uArch/RTL as abstraction
  - uarch/RTL necessitates: we must use then uarch or rtl sim
  - uarch sim is inaccurate: we must use rtl sim
  - rtl sim is slow: so what can we do about it?
  - so we must use some kind of sampled rtl sim with functional uarch warmup models ganged with a functional isa model
-->
